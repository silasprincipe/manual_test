{
  "hash": "a846a8bcbba2d8274ea059a0a8590a42",
  "result": {
    "markdown": "---\ntitle: \"Using the Parquet format with OBIS data\"\ndescription: \"Working with large datasets can be hard due to memory constraints, but using Parquet files can make it possible.\"\nauthor: \"Silas Principe\"\ndate: 2023-08-01\ncategories: \n  - R\n  - Data manipulation\n---\n\n\n# What is Parquet?\n\nParquet is a lightweight format designed for columnar storage. Its main difference when compared to other formats like __csv__ is that Parquet is column-oriented (while csv is row-oriented). This means that Parquet is much more efficient for data accessing.To illustrate, consider the scenario of extracting data from a specific column in a CSV file. This operation entails reading through all rows across all columns. In contrast, Parquet enables selective access solely to the required column, minimizing unnecessary data retrieval. Also very important: Parquet files are several times lighter than csv files, improving storage and sharing of data. You can learn more about Parquet [here](https://parquet.apache.org/).\n\n![image 1](image1.jpg)\n\nThe `Arrow` package enable to work with Parquet files (as well some other interesting formats) within R. You can read the full documentation of the package [here](https://arrow.apache.org/docs/r/index.html).\n\nWe will now see how you can use Parquet in your data analysis workflow. Note that the real advantage of Parquet comes when working with large datasets, specially those that you can't load into memory.\n\n# Reading and writing Parquet files\n\nOpening a Parquet file is similar to opening a csv, and is done through the function `read_parquet`. We will start working with a small dataset containing records from __OBIS__ for a fiddler crab species (*Leptuca thayeri*) which you can download [here](https://raw.githubusercontent.com/iobis/resources/main/content/tutorials/arrow-obis/leptuca_thayeri.parquet).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(arrow) # To open the parquet files\nlibrary(dplyr) # For data manipulation\n\nspecies <- read_parquet(\"leptuca_thayeri.parquet\")\n\nhead(species)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 127\n  basisOfRecord     class       continent country countryCode county datasetName\n  <chr>             <chr>       <chr>     <chr>   <chr>       <chr>  <chr>      \n1 HumanObservation  Malacostra… América … Colomb… CO          San A… Epifauna m…\n2 HumanObservation  Malacostra… América … Colomb… CO          San A… Epifauna m…\n3 PreservedSpecimen Malacostra… <NA>      Brazil  <NA>        Paran… <NA>       \n4 HumanObservation  Malacostra… América … Colomb… CO          San A… Epifauna m…\n5 HumanObservation  Malacostra… América … Colomb… CO          San A… Epifauna m…\n6 HumanObservation  Malacostra… América … Colomb… CO          San A… Epifauna m…\n# ℹ 120 more variables: dateIdentified <chr>, day <chr>, decimalLatitude <dbl>,\n#   decimalLongitude <dbl>, establishmentMeans <chr>, eventDate <chr>,\n#   eventID <chr>, family <chr>, genus <chr>, geodeticDatum <chr>,\n#   georeferenceVerificationStatus <chr>, georeferencedBy <chr>,\n#   georeferencedDate <chr>, habitat <chr>, higherClassification <chr>,\n#   identifiedBy <chr>, identifiedByID <chr>, institutionCode <chr>,\n#   institutionID <chr>, kingdom <chr>, language <chr>, locality <chr>, …\n```\n:::\n:::\n\nAs you can see, the returned object is a `tibble` and you can work with it as any other regular data frame. So, for example, to get all records from Brazil, we can simply use this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspecies_br <- species %>%\n  filter(country == \"Brazil\")\n\nhead(species_br, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 127\n  basisOfRecord     class       continent country countryCode county datasetName\n  <chr>             <chr>       <chr>     <chr>   <chr>       <chr>  <chr>      \n1 PreservedSpecimen Malacostra… <NA>      Brazil  <NA>        Paran… <NA>       \n2 PreservedSpecimen Malacostra… <NA>      Brazil  <NA>        Mucuri <NA>       \n# ℹ 120 more variables: dateIdentified <chr>, day <chr>, decimalLatitude <dbl>,\n#   decimalLongitude <dbl>, establishmentMeans <chr>, eventDate <chr>,\n#   eventID <chr>, family <chr>, genus <chr>, geodeticDatum <chr>,\n#   georeferenceVerificationStatus <chr>, georeferencedBy <chr>,\n#   georeferencedDate <chr>, habitat <chr>, higherClassification <chr>,\n#   identifiedBy <chr>, identifiedByID <chr>, institutionCode <chr>,\n#   institutionID <chr>, kingdom <chr>, language <chr>, locality <chr>, …\n```\n:::\n:::\n\n\nSaving a data frame to Parquet is also simple, and is done through the `write_parquet` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_parquet(species_br, \"leptuca_thayeri_br.parquet\")\n```\n:::\n\n\n# Opening larger-than-memory files\n\nWhile using Parquet files for smaller datasets is also relevant (remember: it's several times lighter!), the real power of Parquet (and `Arrow`) is the ability to work with large datasets without the need to load all the data to memory. Suppose you want to get the number of records available on OBIS for each Teleostei species. This would involve loading all the OBIS database in the memory before filtering the data. If you ever tried that, it's quite probable that your R crashed. However, with Arrow this is a straightforward task.\n\nFor this part of the tutorial, we will work with the full export of the OBIS database which you can download here: https://obis.org/data/access/. The file have ~15GB.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobis_file <- \"obis_20230726.parquet\" # The path to the file\n```\n:::\n\n\nThis time, instead of using `read_parquet` we will use the function `open_dataset`. The function will not read all the file into memory, but will instead read a \"schema\" showing how the file is organized.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobis <- open_dataset(obis_file)\n```\n:::\n\n\nIf you print the `obis` object you will see that it is not a data frame, but instead a `FileSytemDataset` object, showing the columns of the table with their respective data types. So how can we access the data? `Arrow` support `dplyr` verbs that enable us to work with the data without loading it. So in our case we can filter the data as usual:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nteleostei <- obis %>%\n  filter(class == \"Teleostei\") %>%\n  filter(taxonRank == \"Species\") %>%\n  group_by(species) %>%\n  summarise(records = n()) %>%\n  collect()\n\nhead(teleostei)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n  species                records\n  <chr>                    <int>\n1 Halosaurus carinicauda       6\n2 Prognichthys glaphyrae      22\n3 Ogcocephalus nasutus       407\n4 Sebastes constellatus      142\n5 Oxyurichthys lonchotus      41\n6 Malacocottus kincaidi     1772\n```\n:::\n:::\n\n\nDepending on the filters it may take a few seconds before the data is returned. Note that after all the filters we added `collect()`, what indicates to `Arrow` that it should process our request. Several `dplyr` verbs are available to use with `Arrow`, a full list can be found [here](https://arrow.apache.org/docs/dev/r/reference/acero.html).\n\nWhen working with large datasets, it's important that your filter produces an object of reasonable size (i.e., that after `collect()` can be loaded in memory).\n\nIf you need to inspect the data before filtering, its possible to load only a slice of the data with `slice_head`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobis %>%\n  select(class, taxonRank, species) %>% # Select just a few columns\n  slice_head(n = 5) %>% # Select the first 5 lines\n  collect() # Process the request\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 3\n  class     taxonRank species               \n  <chr>     <chr>     <chr>                 \n1 Teleostei Species   Sebastes maliger      \n2 Teleostei Species   Pycnochromis acares   \n3 Teleostei Species   Pycnochromis acares   \n4 Teleostei Species   Pycnochromis acares   \n5 Teleostei Species   Gymnothorax fimbriatus\n```\n:::\n:::\n\n\n# Learning more\n\nThis tutorial barely scratches the surface of the full potential of working with Parquet. For example, it's possible to save Parquet datasets in a way that only certain parts of the data need to be read, what can improve even more the computation. The better place to learn more about `Arrow` is the package website which contain several useful articles - https://arrow.apache.org/docs/dev/r/index.html\n\nYou can also see this tutorial on using Parquet with GBIF data: https://data-blog.gbif.org/post/apache-arrow-and-parquet/",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}