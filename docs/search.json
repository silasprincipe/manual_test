[
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "Packages",
    "section": "",
    "text": "Python packages\n\n\nEnable the download of data from OBIS\n\n\n\n\n\n\n\n\n\n\n\nR packages\n\n\nEnable the download of data from OBIS\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "packages/r/index.html",
    "href": "packages/r/index.html",
    "title": "R packages",
    "section": "",
    "text": "robis\nrobis, our flagship R package, is a client for the OBIS API. It includes functions for data access, as well as a few helper functions for visualizing occurrence data and extracting nested MeasurementOrFact or DNADerivedData records.\nAvailable through CRAN (use install.packages(“robis”)) GitHub: https://github.com/iobis/robis"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nUsing the Parquet format with OBIS data\n\n\n\n\n\nWorking with large datasets can be hard due to memory constraints, but using Parquet files can make it possible.\n\n\n\n\n\n\nAug 1, 2023\n\n\nSilas Principe\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "obis-tutorials",
    "section": "",
    "text": "Go to manual."
  },
  {
    "objectID": "find-dwc.html",
    "href": "find-dwc.html",
    "title": "Find your DwC",
    "section": "",
    "text": "Darwin Core (DwC) is a body of standards (i.e., identifiers, labels, definitions) that facilitate sharing biodiversity informatics. It provides stable terms and vocabularies related to biological objects/data and their collection.\nDwC is meant to be used from the beginning of your research and is an excellent way of organizing and standardizing your data. All data submitted to OBIS must follow the Darwin Core guidelines. Whether you are planning your project or preparing to submit your data to an OBIS node, a key question is on what type of structure your data fits and what DwC terms are relevant (and necessary). Considering that, we prepared this quick tool to explore the option most suited to your case.\nYou may also want to explore our extensive documentation in the OBIS manual, which provide more in-depth details, or this quick decision tree created by Elizabeth Lawrence.\n\nFinding my schema\n\n  \n  \n  \n  \n  \n  \n\n  \n  \n    \n      What kind of data do you have, or will collect?\n       Occurrence\n       Abundance/Percent cover\n       Biomass\n       Habitat\n       Tracking\n       Genetic\n    \n    \n    \n    \n    \n    \n      \n      You don't have genetic data ⇢\n      Will this be a recurring sampling event OR can you aggregate your data in a single event?\n      A recurring sampling is any sampling that will occur in more than one time step. You can also have a single sampling, but have information that is relative to all your data (e.g. all collected on the same place). Note that many datasets can possibly be grouped in an event.\n             Yes\n       No\n    \n\n    \n    \n      \n      You don't have genetic data ⇢\n      This will be a recurring event or can be aggregated into an event ⇢\n      Have (or will) you collect any data associated with samples or sampling?\n      Examples of associated data are temperature, length of specimen, etc.\n             Yes\n       No\n    \n\n    \n    \n      \n      You don't have genetic data ⇢\n      This will be a recurring event or can be aggregated into an event ⇢\n      You collected associated data ⇢\n      \n      + You will also need:\n      \n    \n\n    \n    \n      \n      You don't have genetic data ⇢\n      This will be a recurring event or can be aggregated into an event ⇢\n      You have not collected associated data ⇢\n      \n    \n\n    \n    \n      \n      You don't have genetic data ⇢\n      This will not be a recurring event or can't be aggregated into an event ⇢\n      Have (or will) you collect any data associated with samples or sampling?\n      Examples of associated data are temperature, length of specimen, etc.\n             Yes\n       No\n    \n\n    \n    \n      \n      You don't have genetic data ⇢\n      This will not be a recurring event or can't be aggregated into an event ⇢\n      You collected associated data ⇢\n      \n      + You will also need:\n      \n    \n\n    \n    \n      \n      You don't have genetic data ⇢\n      This will not be a recurring event or can't be aggregated into an event ⇢\n      You have not collected associated data ⇢\n      \n    \n\n    \n    \n    \n    \n      \n      You have genetic data ⇢\n      Have (or will) you collect any data associated with samples or sampling?\n      Examples of associated data are temperature, length of specimen, etc.\n             Yes\n       No\n    \n    \n    \n    \n      \n      You have genetic data ⇢\n      You collected associated data ⇢\n      \n      + You will also need:\n      \n      \n      \n    \n\n    \n    \n      \n      You have genetic data ⇢\n      You have not collected associated data ⇢\n      \n      + You will also need:\n      \n    \n    \n    \n    \n    Restart"
  },
  {
    "objectID": "packages/python/index.html",
    "href": "packages/python/index.html",
    "title": "Python packages",
    "section": "",
    "text": "robis\nrobis, our flagship R package, is a client for the OBIS API. It includes functions for data access, as well as a few helper functions for visualizing occurrence data and extracting nested MeasurementOrFact or DNADerivedData records.\nAvailable through CRAN (use install.packages(“robis”)) GitHub: https://github.com/iobis/robis"
  },
  {
    "objectID": "tutorials/arrow-obis-2023-09-08/index.html",
    "href": "tutorials/arrow-obis-2023-09-08/index.html",
    "title": "Using the Parquet format with OBIS data",
    "section": "",
    "text": "What is Parquet?\nParquet is a lightweight format designed for columnar storage. Its main difference when compared to other formats like csv is that Parquet is column-oriented (while csv is row-oriented). This means that Parquet is much more efficient for data accessing.To illustrate, consider the scenario of extracting data from a specific column in a CSV file. This operation entails reading through all rows across all columns. In contrast, Parquet enables selective access solely to the required column, minimizing unnecessary data retrieval. Also very important: Parquet files are several times lighter than csv files, improving storage and sharing of data. You can learn more about Parquet here.\n\n\n\nimage 1\n\n\nThe Arrow package enable to work with Parquet files (as well some other interesting formats) within R. You can read the full documentation of the package here.\nWe will now see how you can use Parquet in your data analysis workflow. Note that the real advantage of Parquet comes when working with large datasets, specially those that you can’t load into memory.\n\n\nReading and writing Parquet files\nOpening a Parquet file is similar to opening a csv, and is done through the function read_parquet. We will start working with a small dataset containing records from OBIS for a fiddler crab species (Leptuca thayeri) which you can download here.\n\nlibrary(arrow) # To open the parquet files\nlibrary(dplyr) # For data manipulation\n\nspecies &lt;- read_parquet(\"leptuca_thayeri.parquet\")\n\nhead(species)\n\n# A tibble: 6 × 127\n  basisOfRecord     class       continent country countryCode county datasetName\n  &lt;chr&gt;             &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;      \n1 HumanObservation  Malacostra… América … Colomb… CO          San A… Epifauna m…\n2 HumanObservation  Malacostra… América … Colomb… CO          San A… Epifauna m…\n3 PreservedSpecimen Malacostra… &lt;NA&gt;      Brazil  &lt;NA&gt;        Paran… &lt;NA&gt;       \n4 HumanObservation  Malacostra… América … Colomb… CO          San A… Epifauna m…\n5 HumanObservation  Malacostra… América … Colomb… CO          San A… Epifauna m…\n6 HumanObservation  Malacostra… América … Colomb… CO          San A… Epifauna m…\n# ℹ 120 more variables: dateIdentified &lt;chr&gt;, day &lt;chr&gt;, decimalLatitude &lt;dbl&gt;,\n#   decimalLongitude &lt;dbl&gt;, establishmentMeans &lt;chr&gt;, eventDate &lt;chr&gt;,\n#   eventID &lt;chr&gt;, family &lt;chr&gt;, genus &lt;chr&gt;, geodeticDatum &lt;chr&gt;,\n#   georeferenceVerificationStatus &lt;chr&gt;, georeferencedBy &lt;chr&gt;,\n#   georeferencedDate &lt;chr&gt;, habitat &lt;chr&gt;, higherClassification &lt;chr&gt;,\n#   identifiedBy &lt;chr&gt;, identifiedByID &lt;chr&gt;, institutionCode &lt;chr&gt;,\n#   institutionID &lt;chr&gt;, kingdom &lt;chr&gt;, language &lt;chr&gt;, locality &lt;chr&gt;, …\n\n\nAs you can see, the returned object is a tibble and you can work with it as any other regular data frame. So, for example, to get all records from Brazil, we can simply use this:\n\nspecies_br &lt;- species %&gt;%\n  filter(country == \"Brazil\")\n\nhead(species_br, 2)\n\n# A tibble: 2 × 127\n  basisOfRecord     class       continent country countryCode county datasetName\n  &lt;chr&gt;             &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;      \n1 PreservedSpecimen Malacostra… &lt;NA&gt;      Brazil  &lt;NA&gt;        Paran… &lt;NA&gt;       \n2 PreservedSpecimen Malacostra… &lt;NA&gt;      Brazil  &lt;NA&gt;        Mucuri &lt;NA&gt;       \n# ℹ 120 more variables: dateIdentified &lt;chr&gt;, day &lt;chr&gt;, decimalLatitude &lt;dbl&gt;,\n#   decimalLongitude &lt;dbl&gt;, establishmentMeans &lt;chr&gt;, eventDate &lt;chr&gt;,\n#   eventID &lt;chr&gt;, family &lt;chr&gt;, genus &lt;chr&gt;, geodeticDatum &lt;chr&gt;,\n#   georeferenceVerificationStatus &lt;chr&gt;, georeferencedBy &lt;chr&gt;,\n#   georeferencedDate &lt;chr&gt;, habitat &lt;chr&gt;, higherClassification &lt;chr&gt;,\n#   identifiedBy &lt;chr&gt;, identifiedByID &lt;chr&gt;, institutionCode &lt;chr&gt;,\n#   institutionID &lt;chr&gt;, kingdom &lt;chr&gt;, language &lt;chr&gt;, locality &lt;chr&gt;, …\n\n\nSaving a data frame to Parquet is also simple, and is done through the write_parquet function:\n\nwrite_parquet(species_br, \"leptuca_thayeri_br.parquet\")\n\n\n\nOpening larger-than-memory files\nWhile using Parquet files for smaller datasets is also relevant (remember: it’s several times lighter!), the real power of Parquet (and Arrow) is the ability to work with large datasets without the need to load all the data to memory. Suppose you want to get the number of records available on OBIS for each Teleostei species. This would involve loading all the OBIS database in the memory before filtering the data. If you ever tried that, it’s quite probable that your R crashed. However, with Arrow this is a straightforward task.\nFor this part of the tutorial, we will work with the full export of the OBIS database which you can download here: https://obis.org/data/access/. The file have ~15GB.\n\nobis_file &lt;- \"obis_20230726.parquet\" # The path to the file\n\nThis time, instead of using read_parquet we will use the function open_dataset. The function will not read all the file into memory, but will instead read a “schema” showing how the file is organized.\n\nobis &lt;- open_dataset(obis_file)\n\nIf you print the obis object you will see that it is not a data frame, but instead a FileSytemDataset object, showing the columns of the table with their respective data types. So how can we access the data? Arrow support dplyr verbs that enable us to work with the data without loading it. So in our case we can filter the data as usual:\n\nteleostei &lt;- obis %&gt;%\n  filter(class == \"Teleostei\") %&gt;%\n  filter(taxonRank == \"Species\") %&gt;%\n  group_by(species) %&gt;%\n  summarise(records = n()) %&gt;%\n  collect()\n\nhead(teleostei)\n\n# A tibble: 6 × 2\n  species                 records\n  &lt;chr&gt;                     &lt;int&gt;\n1 Sebastes maliger          41380\n2 Pycnochromis acares        8724\n3 Gymnothorax fimbriatus      857\n4 Monotaxis grandoculis     10961\n5 Ctenochaetus flavicauda    1297\n6 Sargocentron tiere         3687\n\n\nDepending on the filters it may take a few seconds before the data is returned. Note that after all the filters we added collect(), what indicates to Arrow that it should process our request. Several dplyr verbs are available to use with Arrow, a full list can be found here.\nWhen working with large datasets, it’s important that your filter produces an object of reasonable size (i.e., that after collect() can be loaded in memory).\nIf you need to inspect the data before filtering, its possible to load only a slice of the data with slice_head:\n\nobis %&gt;%\n  select(class, taxonRank, species) %&gt;% # Select just a few columns\n  slice_head(n = 5) %&gt;% # Select the first 5 lines\n  collect() # Process the request\n\n# A tibble: 5 × 3\n  class     taxonRank species               \n  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;                 \n1 Teleostei Species   Sebastes maliger      \n2 Teleostei Species   Pycnochromis acares   \n3 Teleostei Species   Pycnochromis acares   \n4 Teleostei Species   Pycnochromis acares   \n5 Teleostei Species   Gymnothorax fimbriatus\n\n\n\n\nLearning more\nThis tutorial barely scratches the surface of the full potential of working with Parquet. For example, it’s possible to save Parquet datasets in a way that only certain parts of the data need to be read, what can improve even more the computation. The better place to learn more about Arrow is the package website which contain several useful articles - https://arrow.apache.org/docs/dev/r/index.html\nYou can also see this tutorial on using Parquet with GBIF data: https://data-blog.gbif.org/post/apache-arrow-and-parquet/"
  }
]